{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Mental Health Signal Detection in Arabic Tweets\n",
    "## A Comprehensive Research Framework\n",
    "\n",
    "### Research Objectives:\n",
    "1. **Primary Goal**: Develop a robust multi-class classification system for detecting mental health signals in Arabic social media text\n",
    "2. **Secondary Goals**:\n",
    "   - Address class imbalance through advanced techniques\n",
    "   - Leverage Arabic-specific NLP models and preprocessing\n",
    "   - Implement interpretable ML approaches for clinical validity\n",
    "   - Ensure ethical considerations in mental health AI applications\n",
    "\n",
    "### Key Improvements:\n",
    "- Arabic-specific text preprocessing and normalization\n",
    "- Advanced feature engineering including psycholinguistic features\n",
    "- State-of-the-art transformer models (AraBERT, CAMeLBERT)\n",
    "- Proper handling of severe class imbalance\n",
    "- Rigorous evaluation with clinical relevance metrics\n",
    "- Model interpretability and error analysis\n",
    "- Ethical considerations and bias detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "Transformers version: 4.43.4\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Data handling\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Fix emoji library compatibility issue\n",
    "try:\n",
    "    import emoji\n",
    "    # For emoji 2.0+, use the new API\n",
    "    if hasattr(emoji, 'EMOJI_DATA'):\n",
    "        EMOJI_DATA = emoji.EMOJI_DATA\n",
    "    else:\n",
    "        # For newer versions, emoji functionality is handled differently\n",
    "        EMOJI_DATA = None\n",
    "except ImportError:\n",
    "    EMOJI_DATA = None\n",
    "\n",
    "# Arabic NLP libraries\n",
    "import pyarabic.araby as araby\n",
    "import camel_tools\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "from arabert import ArabertPreprocessor\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, GridSearchCV, \n",
    "    RandomizedSearchCV, cross_val_score, cross_validate\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    precision_recall_curve, roc_curve, auc, matthews_corrcoef,\n",
    "    cohen_kappa_score, make_scorer\n",
    ")\n",
    "\n",
    "# Advanced ML algorithms\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Class imbalance handling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# Model interpretation\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# Visualization\n",
    "from wordcloud import WordCloud\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET OVERVIEW\n",
      "================================================================================\n",
      "Total samples: 48,856\n",
      "Total features: 39\n",
      "\n",
      "Class Distribution:\n",
      "  neutral             : 42,931 (90.14%)\n",
      "  depression          :  3,206 ( 6.73%)\n",
      "  anxiety             :    939 ( 1.97%)\n",
      "  suicidal_ideation   :    549 ( 1.15%)\n",
      "\n",
      "Class Imbalance Ratio: 78.20:1\n",
      "Minority class: suicidal_ideation (549 samples)\n",
      "Majority class: neutral (42931 samples)\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECKS\n",
      "================================================================================\n",
      "Missing values in 'text': 0\n",
      "Duplicate texts: 0\n",
      "Empty texts: 0\n",
      "\n",
      "Text Length Statistics:\n",
      "count    48856.00000\n",
      "mean        92.09835\n",
      "std         72.44827\n",
      "min         15.00000\n",
      "25%         36.00000\n",
      "50%         65.00000\n",
      "75%        128.00000\n",
      "max        280.00000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('datasets/arabic_tweets_matched_classifications.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "class_dist = df['classification'].value_counts()\n",
    "class_pct = df['classification'].value_counts(normalize=True) * 100\n",
    "\n",
    "for cls in class_dist.index:\n",
    "    print(f\"  {cls:20s}: {class_dist[cls]:6,} ({class_pct[cls]:5.2f}%)\")\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "imbalance_ratio = class_dist.max() / class_dist.min()\n",
    "print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Minority class: {class_dist.idxmin()} ({class_dist.min()} samples)\")\n",
    "print(f\"Majority class: {class_dist.idxmax()} ({class_dist.max()} samples)\")\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Missing values in 'text': {df['text'].isna().sum()}\")\n",
    "print(f\"Duplicate texts: {df['text'].duplicated().sum()}\")\n",
    "print(f\"Empty texts: {(df['text'].str.strip() == '').sum()}\")\n",
    "\n",
    "# Text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "#FF6B6B",
           "#4ECDC4",
           "#45B7D1",
           "#FFA07A"
          ]
         },
         "showlegend": false,
         "text": {
          "bdata": "AAAAAGD25EAAAAAAAAypQAAAAAAAWI1AAAAAAAAogUA=",
          "dtype": "f8"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "neutral",
          "depression",
          "anxiety",
          "suicidal_ideation"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "s6cAAIYMAACrAwAAJQIAAA==",
          "dtype": "i4"
         },
         "yaxis": "y"
        },
        {
         "marker": {
          "color": [
           "#FF6B6B",
           "#4ECDC4",
           "#45B7D1",
           "#FFA07A"
          ]
         },
         "showlegend": false,
         "text": {
          "bdata": "AAAAAGD25EAAAAAAAAypQAAAAAAAWI1AAAAAAAAogUA=",
          "dtype": "f8"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "neutral",
          "depression",
          "anxiety",
          "suicidal_ideation"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "s6cAAIYMAACrAwAAJQIAAA==",
          "dtype": "i4"
         },
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Class Distribution",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Class Distribution (Log Scale)",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 400,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Mental Health Classification Distribution"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Class"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ],
         "title": {
          "text": "Class"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Count"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Count (Log Scale)"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize class distribution\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Class Distribution', 'Class Distribution (Log Scale)'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# Regular scale\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=class_dist.index,\n",
    "        y=class_dist.values,\n",
    "        text=class_dist.values,\n",
    "        textposition='outside',\n",
    "        marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'],\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Log scale\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=class_dist.index,\n",
    "        y=class_dist.values,\n",
    "        text=class_dist.values,\n",
    "        textposition='outside',\n",
    "        marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'],\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count (Log Scale)\", type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Class\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Class\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Mental Health Classification Distribution\",\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Arabic Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Optional: Using Complement NB with TF-IDF Only Features\n",
    "\n",
    "If you want to use Complement Naive Bayes, it requires **non-negative features**. You can use it with TF-IDF features only (which are always non-negative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Train Complement NB with TF-IDF only features (non-negative)\n",
    "# Uncomment to run:\n",
    "\n",
    "# complement_nb_model = ComplementNB(alpha=1.0)\n",
    "# complement_nb_model.fit(X_train_tfidf_only, y_train)\n",
    "# y_pred_cnb = complement_nb_model.predict(X_test_tfidf_only)\n",
    "# \n",
    "# print(\"Complement NB Performance (TF-IDF only):\")\n",
    "# print(f\"F1 Weighted: {f1_score(y_test, y_pred_cnb, average='weighted'):.4f}\")\n",
    "# print(f\"F1 Macro: {f1_score(y_test, y_pred_cnb, average='macro'):.4f}\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred_cnb):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic text preprocessor initialized\n"
     ]
    }
   ],
   "source": [
    "class ArabicTextPreprocessor:\n",
    "    \"\"\"Advanced Arabic text preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 remove_diacritics=True,\n",
    "                 normalize_arabic=True,\n",
    "                 remove_punctuation=True,\n",
    "                 remove_english=False,\n",
    "                 remove_numbers=False,\n",
    "                 remove_extra_spaces=True,\n",
    "                 apply_stemming=False,\n",
    "                 apply_segmentation=False):\n",
    "        \n",
    "        self.remove_diacritics = remove_diacritics\n",
    "        self.normalize_arabic = normalize_arabic\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_english = remove_english\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.remove_extra_spaces = remove_extra_spaces\n",
    "        self.apply_stemming = apply_stemming\n",
    "        self.apply_segmentation = apply_segmentation\n",
    "        \n",
    "        # Initialize Arabic NLP tools\n",
    "        if self.apply_stemming:\n",
    "            self.stemmer = FarasaStemmer()\n",
    "        if self.apply_segmentation:\n",
    "            self.segmenter = FarasaSegmenter()\n",
    "        \n",
    "        # Arabic-specific patterns\n",
    "        self.arabic_punct = 'ÿåÿõÿü'\n",
    "        self.emoticons_pattern = re.compile(r'[:;=][-~]?[)D(|\\/@]|[)D(|\\/@][-~]?[:;=]')\n",
    "        \n",
    "        # Mental health keywords (for feature extraction)\n",
    "        self.depression_keywords = [\n",
    "            'ÿ≠ÿ≤ŸÜ', 'ÿßŸÉÿ™ÿ¶ÿßÿ®', 'Ÿäÿ£ÿ≥', 'Ÿàÿ≠ÿØÿ©', 'ÿ£ŸÑŸÖ', 'ÿ®ŸÉÿßÿ°', 'ÿØŸÖŸàÿπ',\n",
    "            'ŸÅÿ±ÿßÿ∫', 'ÿ∂Ÿäÿßÿπ', 'ÿ™ÿπÿ®', 'ÿ•ÿ±ŸáÿßŸÇ', 'ÿßŸÜŸáŸäÿßÿ±', 'ŸÉÿ≥ÿ±', 'ŸÖŸàÿ™'\n",
    "        ]\n",
    "        self.anxiety_keywords = [\n",
    "            'ŸÇŸÑŸÇ', 'ÿÆŸàŸÅ', 'ÿ™Ÿàÿ™ÿ±', 'ÿ∂ÿ∫ÿ∑', 'ŸáŸÑÿπ', 'ÿ±ÿπÿ®', 'ÿßÿ∂ÿ∑ÿ±ÿßÿ®',\n",
    "            'ÿπÿµÿ®Ÿäÿ©', 'ÿßÿ±ÿ™ÿ®ÿßŸÉ', 'ÿ∞ÿπÿ±', 'ŸÅÿ≤ÿπ', 'ÿ±Ÿáÿßÿ®'\n",
    "        ]\n",
    "        self.suicide_keywords = [\n",
    "            'ÿßŸÜÿ™ÿ≠ÿßÿ±', 'ŸÖŸàÿ™', 'ŸÇÿ™ŸÑ ŸÜŸÅÿ≥', 'ŸÜŸáÿßŸäÿ©', 'ŸàÿØÿßÿπ', 'ÿ£ÿ∞Ÿâ',\n",
    "            'ÿ¨ÿ±ÿ≠', 'ÿ£ŸÑŸÖ ÿ¥ÿØŸäÿØ', 'ŸÑÿß ÿ£ÿ±ŸäÿØ ÿßŸÑÿπŸäÿ¥', 'ÿ≥ÿ¶ŸÖÿ™ ÿßŸÑÿ≠Ÿäÿßÿ©'\n",
    "        ]\n",
    "        \n",
    "    def normalize_text(self, text):\n",
    "        \"\"\"Normalize Arabic text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        # Normalize Arabic characters\n",
    "        if self.normalize_arabic:\n",
    "            text = normalize_alef_ar(text)\n",
    "            text = normalize_alef_maksura_ar(text)\n",
    "            text = normalize_teh_marbuta_ar(text)\n",
    "            \n",
    "            # Additional normalizations\n",
    "            text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
    "            text = re.sub(r'Ÿâ', 'Ÿä', text)\n",
    "            text = re.sub(r'ÿ©', 'Ÿá', text)\n",
    "            text = re.sub(r'⁄Ø', 'ŸÉ', text)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text with various options\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Remove mentions and hashtags (but keep the text)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "        \n",
    "        # Remove diacritics\n",
    "        if self.remove_diacritics:\n",
    "            text = araby.strip_diacritics(text)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation + self.arabic_punct))\n",
    "            \n",
    "        # Remove English characters\n",
    "        if self.remove_english:\n",
    "            text = re.sub(r'[a-zA-Z]', '', text)\n",
    "            \n",
    "        # Remove numbers\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            \n",
    "        # Remove extra spaces\n",
    "        if self.remove_extra_spaces:\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extract psycholinguistic features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Emotional indicators\n",
    "        features['depression_score'] = sum(1 for word in self.depression_keywords if word in text)\n",
    "        features['anxiety_score'] = sum(1 for word in self.anxiety_keywords if word in text)\n",
    "        features['suicide_score'] = sum(1 for word in self.suicide_keywords if word in text)\n",
    "        \n",
    "        # Text characteristics\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('ÿü') + text.count('?')\n",
    "        features['ellipsis_count'] = len(re.findall(r'\\.{2,}', text))\n",
    "        features['caps_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "        \n",
    "        # Emoticon detection\n",
    "        features['emoticon_count'] = len(self.emoticons_pattern.findall(text))\n",
    "        \n",
    "        # Repetition patterns\n",
    "        features['char_repetition'] = len(re.findall(r'(.)\\1{2,}', text))\n",
    "        features['word_repetition'] = self._count_repeated_words(text)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _count_repeated_words(self, text):\n",
    "        \"\"\"Count repeated words in text\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return 0\n",
    "        return sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Main preprocessing pipeline\"\"\"\n",
    "        # Clean and normalize\n",
    "        text = self.clean_text(text)\n",
    "        text = self.normalize_text(text)\n",
    "        \n",
    "        # Apply stemming if requested\n",
    "        if self.apply_stemming and hasattr(self, 'stemmer'):\n",
    "            text = self.stemmer.stem(text)\n",
    "            \n",
    "        # Apply segmentation if requested  \n",
    "        if self.apply_segmentation and hasattr(self, 'segmenter'):\n",
    "            text = self.segmenter.segment(text)\n",
    "            \n",
    "        return text\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = ArabicTextPreprocessor(\n",
    "    remove_diacritics=True,\n",
    "    normalize_arabic=True,\n",
    "    remove_punctuation=False,  # Keep for feature extraction\n",
    "    remove_english=False,\n",
    "    remove_numbers=False\n",
    ")\n",
    "\n",
    "print(\"Arabic text preprocessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd05fd091a9048b3bcd7bc892ab3ec16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting psycholinguistic features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a802740f6f4542890e1b0b5613fcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Added 10 psycholinguistic features\n",
      "\n",
      "Sample preprocessed texts:\n",
      "\n",
      "Original: ŸÖÿ∞ÿßŸÉÿ±Ÿá ÿßÿÆÿ± ÿßÿÆÿ™ÿ®ÿßÿ± ŸáŸä ÿßÿµÿπÿ® ÿ¥Ÿä ÿßŸÖÿ± ŸÅŸäŸá ÿ≠ÿßŸÑŸäÿß...\n",
      "Processed: ŸÖÿ∞ÿßŸÉÿ±Ÿá ÿßÿÆÿ± ÿßÿÆÿ™ÿ®ÿßÿ± ŸáŸä ÿßÿµÿπÿ® ÿ¥Ÿä ÿßŸÖÿ± ŸÅŸäŸá ÿ≠ÿßŸÑŸäÿß...\n",
      "Class: nan\n",
      "\n",
      "Original: ÿ®ÿßÿ±ŸäŸÑŸÑÿß ÿµÿßÿ±ŸÑŸá ŸÉŸÖ ŸÖÿ®ÿßÿ±ÿßŸá ŸÖÿ≥ÿ™ŸàÿßŸá ÿ≥Ÿäÿ° ŸÅŸäÿØÿßŸÑ ÿ®ÿ≥ Ÿäÿ∂ÿ±ÿ® ÿ®ÿ±Ÿàÿ≤Ÿà ŸÖÿ≤ÿßÿ¨Ÿá ÿ≤Ÿä ÿßŸÑÿ≤ŸÅÿ™ ÿßÿÆÿ± ŸÉŸÖ ŸÖÿ®ÿßÿ±ÿßŸá Ÿàÿ≥ÿ∑ ÿπŸÇŸäŸÖ ÿ¨ÿØÿß...\n",
      "Processed: ÿ®ÿßÿ±ŸäŸÑŸÑÿß ÿµÿßÿ±ŸÑŸá ŸÉŸÖ ŸÖÿ®ÿßÿ±ÿßŸá ŸÖÿ≥ÿ™ŸàÿßŸá ÿ≥Ÿäÿ° ŸÅŸäÿØÿßŸÑ ÿ®ÿ≥ Ÿäÿ∂ÿ±ÿ® ÿ®ÿ±Ÿàÿ≤Ÿà ŸÖÿ≤ÿßÿ¨Ÿá ÿ≤Ÿä ÿßŸÑÿ≤ŸÅÿ™ ÿßÿÆÿ± ŸÉŸÖ ŸÖÿ®ÿßÿ±ÿßŸá Ÿàÿ≥ÿ∑ ÿπŸÇŸäŸÖ ÿ¨ÿØÿß...\n",
      "Class: neutral\n",
      "\n",
      "Original: ŸÖÿπÿ±ŸàŸÅŸá ÿßŸÜŸÜÿß ÿ®ŸÜÿ™ÿ≤ŸÜŸÇ ŸÅŸä ÿßÿÆÿ± ÿßÿ≥ÿ®Ÿàÿπ ŸÖŸÜ ÿßŸÑŸÖÿØŸäŸàŸÑ...\n",
      "Processed: ŸÖÿπÿ±ŸàŸÅŸá ÿßŸÜŸÜÿß ÿ®ŸÜÿ™ÿ≤ŸÜŸÇ ŸÅŸä ÿßÿÆÿ± ÿßÿ≥ÿ®Ÿàÿπ ŸÖŸÜ ÿßŸÑŸÖÿØŸäŸàŸÑ...\n",
      "Class: nan\n"
     ]
    }
   ],
   "source": [
    "# Preprocess texts and extract features\n",
    "print(\"Preprocessing texts...\")\n",
    "df['processed_text'] = df['text'].progress_apply(preprocessor.preprocess)\n",
    "\n",
    "print(\"Extracting psycholinguistic features...\")\n",
    "psych_features = df['text'].progress_apply(preprocessor.extract_features)\n",
    "psych_features_df = pd.DataFrame(list(psych_features))\n",
    "\n",
    "# Combine with existing features\n",
    "df = pd.concat([df, psych_features_df], axis=1)\n",
    "\n",
    "print(f\"\\nPreprocessing complete!\")\n",
    "print(f\"Added {len(psych_features_df.columns)} psycholinguistic features\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample preprocessed texts:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['text'].iloc[i][:100]}...\")\n",
    "    print(f\"Processed: {df['processed_text'].iloc[i][:100]}...\")\n",
    "    print(f\"Class: {df['classification'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IProgress in /opt/anaconda3/envs/masters/lib/python3.12/site-packages (0.4)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/masters/lib/python3.12/site-packages (from IProgress) (1.17.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/masters/lib/python3.12/site-packages (from IProgress) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install IProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineer initialized\n"
     ]
    }
   ],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Comprehensive feature engineering for Arabic mental health text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.count_vectorizer = None\n",
    "        self.char_vectorizer = None\n",
    "        \n",
    "    def create_tfidf_features(self, texts, max_features=5000, ngram_range=(1, 3)):\n",
    "        \"\"\"Create TF-IDF features\"\"\"\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            analyzer='word',\n",
    "            min_df=5,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,\n",
    "            use_idf=True\n",
    "        )\n",
    "        return self.tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    def create_char_ngram_features(self, texts, max_features=3000):\n",
    "        \"\"\"Create character n-gram features\"\"\"\n",
    "        self.char_vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=(2, 5),\n",
    "            analyzer='char',\n",
    "            min_df=5,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        return self.char_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    def create_topic_features(self, tfidf_matrix, n_topics=20):\n",
    "        \"\"\"Create topic modeling features using NMF\"\"\"\n",
    "        nmf = NMF(n_components=n_topics, random_state=42)\n",
    "        topic_features = nmf.fit_transform(tfidf_matrix)\n",
    "        return topic_features, nmf\n",
    "    \n",
    "    def create_embedding_features(self, texts, model_name='aubmindlab/bert-base-arabertv2'):\n",
    "        \"\"\"Create contextual embeddings using AraBERT\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        embeddings = []\n",
    "        batch_size = 32\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(texts), batch_size)):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors='pt'\n",
    "                ).to(device)\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                # Use [CLS] token embedding\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def combine_features(self, *feature_arrays):\n",
    "        \"\"\"Combine multiple feature arrays\"\"\"\n",
    "        # Convert sparse matrices to dense if needed\n",
    "        dense_features = []\n",
    "        for feat in feature_arrays:\n",
    "            if hasattr(feat, 'toarray'):\n",
    "                dense_features.append(feat.toarray())\n",
    "            else:\n",
    "                dense_features.append(feat)\n",
    "        \n",
    "        return np.hstack(dense_features)\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "print(\"Feature engineer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature sets...\n",
      "\n",
      "1. Creating TF-IDF features...\n",
      "   TF-IDF shape: (48856, 5000)\n",
      "\n",
      "2. Creating character n-gram features...\n",
      "   TF-IDF shape: (48856, 5000)\n",
      "\n",
      "2. Creating character n-gram features...\n",
      "   Character n-gram shape: (48856, 3000)\n",
      "\n",
      "3. Creating topic features...\n",
      "   Character n-gram shape: (48856, 3000)\n",
      "\n",
      "3. Creating topic features...\n",
      "   Topic features shape: (48856, 20)\n",
      "\n",
      "4. Preparing numerical features...\n",
      "   Numerical features shape: (48856, 33)\n",
      "\n",
      "============================================================\n",
      "Total feature dimensions created: \n",
      "  - TF-IDF: 5000\n",
      "  - Character n-grams: 3000\n",
      "  - Topics: 20\n",
      "  - Numerical: 33\n",
      "============================================================\n",
      "   Topic features shape: (48856, 20)\n",
      "\n",
      "4. Preparing numerical features...\n",
      "   Numerical features shape: (48856, 33)\n",
      "\n",
      "============================================================\n",
      "Total feature dimensions created: \n",
      "  - TF-IDF: 5000\n",
      "  - Character n-grams: 3000\n",
      "  - Topics: 20\n",
      "  - Numerical: 33\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create various feature sets\n",
    "print(\"Creating feature sets...\\n\")\n",
    "\n",
    "# 1. TF-IDF features\n",
    "print(\"1. Creating TF-IDF features...\")\n",
    "tfidf_features = feature_engineer.create_tfidf_features(\n",
    "    df['processed_text'].values,\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "print(f\"   TF-IDF shape: {tfidf_features.shape}\")\n",
    "\n",
    "# 2. Character n-gram features\n",
    "print(\"\\n2. Creating character n-gram features...\")\n",
    "char_features = feature_engineer.create_char_ngram_features(\n",
    "    df['processed_text'].values,\n",
    "    max_features=3000\n",
    ")\n",
    "print(f\"   Character n-gram shape: {char_features.shape}\")\n",
    "\n",
    "# 3. Topic features\n",
    "print(\"\\n3. Creating topic features...\")\n",
    "topic_features, nmf_model = feature_engineer.create_topic_features(\n",
    "    tfidf_features,\n",
    "    n_topics=20\n",
    ")\n",
    "print(f\"   Topic features shape: {topic_features.shape}\")\n",
    "\n",
    "# 4. Numerical features\n",
    "print(\"\\n4. Preparing numerical features...\")\n",
    "numerical_features = [\n",
    "    'char_count', 'word_count', 'sentence_count', 'avg_words_per_sentence',\n",
    "    'mention_count', 'hashtag_count', 'url_count',\n",
    "    'punctuation_count', 'exclamation_count', 'question_count', 'emoji_count',\n",
    "    'arabic_char_count', 'arabic_ratio', 'repeated_chars',\n",
    "    'latin_char_count', 'latin_ratio', 'digit_count', 'digit_ratio',\n",
    "    'unique_words', 'lexical_diversity', 'avg_word_length',\n",
    "    'ellipsis_count', 'space_count',\n",
    "    # Psycholinguistic features\n",
    "    'depression_score', 'anxiety_score', 'suicide_score',\n",
    "    'caps_ratio', 'emoticon_count', 'char_repetition', 'word_repetition'\n",
    "]\n",
    "\n",
    "# Check which features exist\n",
    "available_numerical = [f for f in numerical_features if f in df.columns]\n",
    "numerical_feat_array = df[available_numerical].values\n",
    "print(f\"   Numerical features shape: {numerical_feat_array.shape}\")\n",
    "\n",
    "# 5. Scale numerical features\n",
    "scaler = RobustScaler()\n",
    "numerical_feat_scaled = scaler.fit_transform(numerical_feat_array)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Total feature dimensions created: \")\n",
    "print(f\"  - TF-IDF: {tfidf_features.shape[1]}\")\n",
    "print(f\"  - Character n-grams: {char_features.shape[1]}\")\n",
    "print(f\"  - Topics: {topic_features.shape[1]}\")\n",
    "print(f\"  - Numerical: {numerical_feat_scaled.shape[1]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting and Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding:\n",
      "  0: anxiety\n",
      "  1: depression\n",
      "  2: neutral\n",
      "  3: suicidal_ideation\n",
      "  4: nan\n",
      "\n",
      "Combining feature sets...\n",
      "Combined features shape: (48856, 5053)\n",
      "TF-IDF + Numerical shape: (48856, 5033)\n",
      "Char + Numerical shape: (48856, 3033)\n",
      "Combined features shape: (48856, 5053)\n",
      "TF-IDF + Numerical shape: (48856, 5033)\n",
      "Char + Numerical shape: (48856, 3033)\n",
      "\n",
      "Training set: (39084, 5053)\n",
      "Test set: (9772, 5053)\n",
      "\n",
      "Training label distribution:\n",
      "  anxiety: 751 (1.92%)\n",
      "  depression: 2565 (6.56%)\n",
      "  neutral: 34344 (87.87%)\n",
      "  suicidal_ideation: 439 (1.12%)\n",
      "  nan: 985 (2.52%)\n",
      "\n",
      "Training set: (39084, 5053)\n",
      "Test set: (9772, 5053)\n",
      "\n",
      "Training label distribution:\n",
      "  anxiety: 751 (1.92%)\n",
      "  depression: 2565 (6.56%)\n",
      "  neutral: 34344 (87.87%)\n",
      "  suicidal_ideation: 439 (1.12%)\n",
      "  nan: 985 (2.52%)\n"
     ]
    }
   ],
   "source": [
    "# Prepare labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['classification'])\n",
    "\n",
    "# Create mapping\n",
    "label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), \n",
    "                        label_encoder.classes_))\n",
    "print(\"Label Encoding:\")\n",
    "for idx, label in label_mapping.items():\n",
    "    print(f\"  {idx}: {label}\")\n",
    "\n",
    "# Combine feature sets\n",
    "print(\"\\nCombining feature sets...\")\n",
    "\n",
    "# Option 1: All features combined\n",
    "X_combined = feature_engineer.combine_features(\n",
    "    tfidf_features,\n",
    "    numerical_feat_scaled,\n",
    "    topic_features\n",
    ")\n",
    "\n",
    "# Option 2: TF-IDF + Numerical\n",
    "X_tfidf_num = feature_engineer.combine_features(\n",
    "    tfidf_features,\n",
    "    numerical_feat_scaled\n",
    ")\n",
    "\n",
    "# Option 3: Character n-grams + Numerical\n",
    "X_char_num = feature_engineer.combine_features(\n",
    "    char_features,\n",
    "    numerical_feat_scaled\n",
    ")\n",
    "\n",
    "print(f\"Combined features shape: {X_combined.shape}\")\n",
    "print(f\"TF-IDF + Numerical shape: {X_tfidf_num.shape}\")\n",
    "print(f\"Char + Numerical shape: {X_char_num.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Also split text for deep learning models\n",
    "X_train_text, X_test_text = train_test_split(\n",
    "    df['processed_text'].values, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining label distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  {label_mapping[u]}: {c} ({c/len(y_train)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing class imbalance strategies...\n",
      "\n",
      "Class weights:\n",
      "  anxiety: 10.409\n",
      "  depression: 3.047\n",
      "  neutral: 0.228\n",
      "  suicidal_ideation: 17.806\n",
      "  nan: 7.936\n",
      "\n",
      "Creating resampled datasets...\n",
      "SMOTE: (171720, 5053)\n",
      "SMOTE: (171720, 5053)\n",
      "Borderline-SMOTE: (171720, 5053)\n",
      "Borderline-SMOTE: (171720, 5053)\n",
      "ADASYN: (171474, 5053)\n",
      "ADASYN: (171474, 5053)\n",
      "SMOTE+Tomek: (171714, 5053)\n",
      "\n",
      "Resampled label distributions:\n",
      "\n",
      "SMOTE:\n",
      "  anxiety: 34344\n",
      "  depression: 34344\n",
      "  neutral: 34344\n",
      "  suicidal_ideation: 34344\n",
      "  nan: 34344\n",
      "\n",
      "Borderline-SMOTE:\n",
      "  anxiety: 34344\n",
      "  depression: 34344\n",
      "  neutral: 34344\n",
      "  suicidal_ideation: 34344\n",
      "  nan: 34344\n",
      "\n",
      "ADASYN:\n",
      "  anxiety: 34456\n",
      "  depression: 33665\n",
      "  neutral: 34344\n",
      "  suicidal_ideation: 34423\n",
      "  nan: 34586\n",
      "\n",
      "SMOTE+Tomek:\n",
      "  anxiety: 34344\n",
      "  depression: 34343\n",
      "  neutral: 34341\n",
      "  suicidal_ideation: 34344\n",
      "  nan: 34342\n",
      "SMOTE+Tomek: (171714, 5053)\n",
      "\n",
      "Resampled label distributions:\n",
      "\n",
      "SMOTE:\n",
      "  anxiety: 34344\n",
      "  depression: 34344\n",
      "  neutral: 34344\n",
      "  suicidal_ideation: 34344\n",
      "  nan: 34344\n",
      "\n",
      "Borderline-SMOTE:\n",
      "  anxiety: 34344\n",
      "  depression: 34344\n",
      "  neutral: 34344\n",
      "  suicidal_ideation: 34344\n",
      "  nan: 34344\n",
      "\n",
      "ADASYN:\n",
      "  anxiety: 34456\n",
      "  depression: 33665\n",
      "  neutral: 34344\n",
      "  suicidal_ideation: 34423\n",
      "  nan: 34586\n",
      "\n",
      "SMOTE+Tomek:\n",
      "  anxiety: 34344\n",
      "  depression: 34343\n",
      "  neutral: 34341\n",
      "  suicidal_ideation: 34344\n",
      "  nan: 34342\n"
     ]
    }
   ],
   "source": [
    "# Handle class imbalance with multiple strategies\n",
    "print(\"Implementing class imbalance strategies...\\n\")\n",
    "\n",
    "# 1. Class weights for weighted loss\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(\"Class weights:\")\n",
    "for cls, weight in class_weight_dict.items():\n",
    "    print(f\"  {label_mapping[cls]}: {weight:.3f}\")\n",
    "\n",
    "# 2. SMOTE variants\n",
    "print(\"\\nCreating resampled datasets...\")\n",
    "\n",
    "# Standard SMOTE\n",
    "smote = SMOTE(random_state=RANDOM_SEED, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(f\"SMOTE: {X_train_smote.shape}\")\n",
    "\n",
    "# Borderline SMOTE (better for borderline cases)\n",
    "borderline_smote = BorderlineSMOTE(random_state=RANDOM_SEED, k_neighbors=5)\n",
    "X_train_bsmote, y_train_bsmote = borderline_smote.fit_resample(X_train, y_train)\n",
    "print(f\"Borderline-SMOTE: {X_train_bsmote.shape}\")\n",
    "\n",
    "# ADASYN (adaptive synthetic sampling)\n",
    "adasyn = ADASYN(random_state=RANDOM_SEED, n_neighbors=5)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "print(f\"ADASYN: {X_train_adasyn.shape}\")\n",
    "\n",
    "# Combined approach: SMOTE + Tomek\n",
    "smote_tomek = SMOTETomek(random_state=RANDOM_SEED)\n",
    "X_train_combined, y_train_combined = smote_tomek.fit_resample(X_train, y_train)\n",
    "print(f\"SMOTE+Tomek: {X_train_combined.shape}\")\n",
    "\n",
    "print(\"\\nResampled label distributions:\")\n",
    "for name, y_resampled in [('SMOTE', y_train_smote), \n",
    "                          ('Borderline-SMOTE', y_train_bsmote),\n",
    "                          ('ADASYN', y_train_adasyn),\n",
    "                          ('SMOTE+Tomek', y_train_combined)]:\n",
    "    unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "    print(f\"\\n{name}:\")\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"  {label_mapping[u]}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training - Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 10 models for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Define comprehensive model suite\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    'Ridge Classifier': RidgeClassifier(\n",
    "        alpha=1.0,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    'Linear SVM': LinearSVC(\n",
    "        max_iter=10000,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        class_weights=class_weight_dict,\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbose=False\n",
    "    ),\n",
    "    'MLP': MLPClassifier(\n",
    "        hidden_layer_sizes=(256, 128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        max_iter=500,\n",
    "        random_state=RANDOM_SEED,\n",
    "        early_stopping=True\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(models)} models for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING ON ORIGINAL DATA\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918734d371234709a95ccf74d9aea85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "  CV F1 (weighted): 0.7434 (+/- 0.0041)\n",
      "  Test F1 (weighted): 0.7407\n",
      "  Test F1 (macro): 0.3228\n",
      "\n",
      "Training Ridge Classifier...\n",
      "  CV F1 (weighted): 0.7434 (+/- 0.0041)\n",
      "  Test F1 (weighted): 0.7407\n",
      "  Test F1 (macro): 0.3228\n",
      "\n",
      "Training Ridge Classifier...\n",
      "  CV F1 (weighted): 0.7085 (+/- 0.0050)\n",
      "  Test F1 (weighted): 0.7004\n",
      "  Test F1 (macro): 0.2890\n",
      "\n",
      "Training Linear SVM...\n",
      "  CV F1 (weighted): 0.7085 (+/- 0.0050)\n",
      "  Test F1 (weighted): 0.7004\n",
      "  Test F1 (macro): 0.2890\n",
      "\n",
      "Training Linear SVM...\n",
      "  CV F1 (weighted): 0.8341 (+/- 0.0023)\n",
      "  Test F1 (weighted): 0.8343\n",
      "  Test F1 (macro): 0.3736\n",
      "\n",
      "Training Random Forest...\n",
      "  CV F1 (weighted): 0.8341 (+/- 0.0023)\n",
      "  Test F1 (weighted): 0.8343\n",
      "  Test F1 (macro): 0.3736\n",
      "\n",
      "Training Random Forest...\n",
      "  CV F1 (weighted): 0.7619 (+/- 0.0054)\n",
      "  Test F1 (weighted): 0.7392\n",
      "  Test F1 (macro): 0.2953\n",
      "\n",
      "Training Extra Trees...\n",
      "  CV F1 (weighted): 0.7619 (+/- 0.0054)\n",
      "  Test F1 (weighted): 0.7392\n",
      "  Test F1 (macro): 0.2953\n",
      "\n",
      "Training Extra Trees...\n",
      "  CV F1 (weighted): 0.6761 (+/- 0.0222)\n",
      "  Test F1 (weighted): 0.6696\n",
      "  Test F1 (macro): 0.2652\n",
      "\n",
      "Training XGBoost...\n",
      "  CV F1 (weighted): 0.6761 (+/- 0.0222)\n",
      "  Test F1 (weighted): 0.6696\n",
      "  Test F1 (macro): 0.2652\n",
      "\n",
      "Training XGBoost...\n",
      "  CV F1 (weighted): 0.8429 (+/- 0.0019)\n",
      "  Test F1 (weighted): 0.8419\n",
      "  Test F1 (macro): 0.3107\n",
      "\n",
      "Training LightGBM...\n",
      "  CV F1 (weighted): 0.8429 (+/- 0.0019)\n",
      "  Test F1 (weighted): 0.8419\n",
      "  Test F1 (macro): 0.3107\n",
      "\n",
      "Training LightGBM...\n",
      "  CV F1 (weighted): 0.7780 (+/- 0.0013)\n",
      "  Test F1 (weighted): 0.7742\n",
      "  Test F1 (macro): 0.3317\n",
      "\n",
      "Training CatBoost...\n",
      "  CV F1 (weighted): 0.7780 (+/- 0.0013)\n",
      "  Test F1 (weighted): 0.7742\n",
      "  Test F1 (macro): 0.3317\n",
      "\n",
      "Training CatBoost...\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation function with checkpoint saving\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test, cv_folds=5, checkpoint_dir='models/training_progress'):\n",
    "    \"\"\"Comprehensive model evaluation with cross-validation and checkpoint saving\"\"\"\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, 'ml_models_checkpoint.pkl')\n",
    "    \n",
    "    # Try to load existing checkpoint\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"Found checkpoint at {checkpoint_file}\")\n",
    "        response = input(\"Resume from checkpoint? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            results = joblib.load(checkpoint_file)\n",
    "            completed_models = list(results.keys())\n",
    "            print(f\"‚úÖ Loaded checkpoint with {len(completed_models)} completed models: {completed_models}\")\n",
    "            # Filter out already completed models\n",
    "            models = {k: v for k, v in models.items() if k not in completed_models}\n",
    "            print(f\"Remaining models to train: {list(models.keys())}\")\n",
    "        else:\n",
    "            results = {}\n",
    "    else:\n",
    "        results = {}\n",
    "    \n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1_weighted': 'f1_weighted',\n",
    "        'f1_macro': 'f1_macro',\n",
    "        'precision_weighted': 'precision_weighted',\n",
    "        'recall_weighted': 'recall_weighted'\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for name, model in tqdm(models.items(), desc=\"Training models\"):\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation\n",
    "            cv_results = cross_validate(\n",
    "                model, X_train, y_train,\n",
    "                cv=skf,\n",
    "                scoring=scoring,\n",
    "                return_train_score=True,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Train on full training set\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Test metrics\n",
    "            test_metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "                'f1_macro': f1_score(y_test, y_pred, average='macro'),\n",
    "                'precision_weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "                'recall_weighted': recall_score(y_test, y_pred, average='weighted'),\n",
    "                'matthews_corr': matthews_corrcoef(y_test, y_pred),\n",
    "                'cohen_kappa': cohen_kappa_score(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            # Per-class metrics\n",
    "            class_report = classification_report(\n",
    "                y_test, y_pred,\n",
    "                target_names=label_encoder.classes_,\n",
    "                output_dict=True\n",
    "            )\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'cv_results': cv_results,\n",
    "                'test_metrics': test_metrics,\n",
    "                'class_report': class_report,\n",
    "                'predictions': y_pred,\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            # Save checkpoint after each model\n",
    "            joblib.dump(results, checkpoint_file)\n",
    "            print(f\"üíæ Checkpoint saved ({len(results)}/{len(models) + len(results)} models completed)\")\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"  CV F1 (weighted): {cv_results['test_f1_weighted'].mean():.4f} (+/- {cv_results['test_f1_weighted'].std():.4f})\")\n",
    "            print(f\"  Test F1 (weighted): {test_metrics['f1_weighted']:.4f}\")\n",
    "            print(f\"  Test F1 (macro): {test_metrics['f1_macro']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training {name}: {str(e)}\")\n",
    "            print(f\"   Skipping this model and continuing...\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train models on different sampling strategies\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING ON ORIGINAL DATA\")\n",
    "print(\"=\"*80)\n",
    "results_original = evaluate_models(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING ON SMOTE RESAMPLED DATA\")\n",
    "print(\"=\"*80)\n",
    "results_smote = evaluate_models(models, X_train_smote, y_train_smote, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "def create_results_comparison(results_dict):\n",
    "    \"\"\"Create comparison dataframe from results\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for strategy_name, results in results_dict.items():\n",
    "        for model_name, model_results in results.items():\n",
    "            row = {\n",
    "                'Strategy': strategy_name,\n",
    "                'Model': model_name,\n",
    "                'CV_F1_Weighted': model_results['cv_results']['test_f1_weighted'].mean(),\n",
    "                'CV_F1_Std': model_results['cv_results']['test_f1_weighted'].std(),\n",
    "                'Test_Accuracy': model_results['test_metrics']['accuracy'],\n",
    "                'Test_F1_Weighted': model_results['test_metrics']['f1_weighted'],\n",
    "                'Test_F1_Macro': model_results['test_metrics']['f1_macro'],\n",
    "                'Matthews_Corr': model_results['test_metrics']['matthews_corr'],\n",
    "                'Cohen_Kappa': model_results['test_metrics']['cohen_kappa']\n",
    "            }\n",
    "            \n",
    "            # Add per-class F1 scores\n",
    "            for cls in label_encoder.classes_:\n",
    "                row[f'F1_{cls}'] = model_results['class_report'][cls]['f1-score']\n",
    "            \n",
    "            comparison_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison\n",
    "all_results = {\n",
    "    'Original': results_original,\n",
    "    'SMOTE': results_smote\n",
    "}\n",
    "\n",
    "comparison_df = create_results_comparison(all_results)\n",
    "\n",
    "# Display best models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP PERFORMING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by macro F1 (better for imbalanced data)\n",
    "top_models = comparison_df.nlargest(10, 'Test_F1_Macro')\n",
    "\n",
    "display_cols = ['Strategy', 'Model', 'Test_F1_Macro', 'Test_F1_Weighted', \n",
    "                'F1_depression', 'F1_anxiety', 'F1_suicidal_ideation']\n",
    "\n",
    "print(top_models[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Learning Models - Transformer-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class for transformers\n",
    "class ArabicMentalHealthDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Fine-tune AraBERT with checkpoint saving\n",
    "def train_arabert(X_train_text, y_train, X_test_text, y_test, \n",
    "                  model_name='aubmindlab/bert-base-arabertv2',\n",
    "                  epochs=5, batch_size=32, checkpoint_dir='models/training_progress'):\n",
    "    \n",
    "    print(f\"Loading {model_name}...\")\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'arabert_checkpoint.pt')\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    start_epoch = 0\n",
    "    training_stats = []\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Found checkpoint at {checkpoint_path}\")\n",
    "        response = input(\"Resume from checkpoint? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            training_stats = checkpoint['training_stats']\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if start_epoch > 0 and os.path.exists(checkpoint_path):\n",
    "        # Load from checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_encoder.classes_),\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"‚úÖ Loaded model weights from checkpoint\")\n",
    "    else:\n",
    "        # Load fresh model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_encoder.classes_),\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ArabicMentalHealthDataset(X_train_text, y_train, tokenizer)\n",
    "    test_dataset = ArabicMentalHealthDataset(X_test_text, y_test, tokenizer)\n",
    "    \n",
    "    # Create weighted sampler for imbalanced data\n",
    "    class_counts = np.bincount(y_train)\n",
    "    weights = 1.0 / class_counts\n",
    "    sample_weights = weights[y_train]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Load optimizer and scheduler state if resuming\n",
    "    if start_epoch > 0 and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"‚úÖ Loaded optimizer and scheduler states\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels).item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'acc': correct_predictions / total_predictions\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs.logits, dim=1)\n",
    "                val_predictions.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        val_f1_weighted = f1_score(val_labels, val_predictions, average='weighted')\n",
    "        val_f1_macro = f1_score(val_labels, val_predictions, average='macro')\n",
    "        \n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val F1 (weighted): {val_f1_weighted:.4f}, Val F1 (macro): {val_f1_macro:.4f}\")\n",
    "        \n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_acc': train_accuracy,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_f1_weighted': val_f1_weighted,\n",
    "            'val_f1_macro': val_f1_macro\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'training_stats': training_stats,\n",
    "            'val_f1_macro': val_f1_macro\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"üíæ Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    return model, training_stats, val_predictions\n",
    "\n",
    "# Train AraBERT\n",
    "print(\"Training AraBERT model...\")\n",
    "arabert_model, arabert_stats, arabert_predictions = train_arabert(\n",
    "    X_train_text, y_train, X_test_text, y_test,\n",
    "    epochs=3  # Reduce for demonstration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model for interpretation\n",
    "best_model_name = comparison_df.nlargest(1, 'Test_F1_Macro')['Model'].values[0]\n",
    "best_strategy = comparison_df.nlargest(1, 'Test_F1_Macro')['Strategy'].values[0]\n",
    "best_model = all_results[best_strategy][best_model_name]['model']\n",
    "best_predictions = all_results[best_strategy][best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best model: {best_model_name} with {best_strategy} strategy\")\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature names\n",
    "    tfidf_names = feature_engineer.tfidf_vectorizer.get_feature_names_out()\n",
    "    feature_names = list(tfidf_names) + available_numerical + [f'topic_{i}' for i in range(20)]\n",
    "    \n",
    "    # Get importances\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Sort and select top features\n",
    "    indices = np.argsort(importances)[::-1][:30]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(f'Top 30 Feature Importances - {best_model_name}')\n",
    "    plt.bar(range(30), importances[indices])\n",
    "    plt.xticks(range(30), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Misclassified samples\n",
    "misclassified_mask = y_test != best_predictions\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "print(f\"Total misclassifications: {len(misclassified_indices)} ({len(misclassified_indices)/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Analyze misclassifications by class\n",
    "for true_class in range(len(label_encoder.classes_)):\n",
    "    class_mask = y_test == true_class\n",
    "    class_misclassified = misclassified_mask & class_mask\n",
    "    \n",
    "    if class_misclassified.sum() > 0:\n",
    "        print(f\"\\n{label_encoder.classes_[true_class]}:\")\n",
    "        print(f\"  Misclassified: {class_misclassified.sum()} / {class_mask.sum()} ({class_misclassified.sum()/class_mask.sum()*100:.2f}%)\")\n",
    "        \n",
    "        # Most common misclassification\n",
    "        pred_for_class = best_predictions[class_mask]\n",
    "        true_for_class = y_test[class_mask]\n",
    "        wrong_preds = pred_for_class[pred_for_class != true_for_class]\n",
    "        \n",
    "        if len(wrong_preds) > 0:\n",
    "            most_common_error = Counter(wrong_preds).most_common(1)[0]\n",
    "            print(f\"  Most confused with: {label_encoder.classes_[most_common_error[0]]} ({most_common_error[1]} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clinical Validity Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical metrics for mental health detection\n",
    "def calculate_clinical_metrics(y_true, y_pred, positive_classes=['depression', 'anxiety', 'suicidal_ideation']):\n",
    "    \"\"\"Calculate clinically relevant metrics\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Convert to binary: any mental health issue vs neutral\n",
    "    positive_indices = [label_encoder.transform([cls])[0] for cls in positive_classes]\n",
    "    y_true_binary = np.isin(y_true, positive_indices).astype(int)\n",
    "    y_pred_binary = np.isin(y_pred, positive_indices).astype(int)\n",
    "    \n",
    "    # Sensitivity (recall for positive class)\n",
    "    results['sensitivity'] = recall_score(y_true_binary, y_pred_binary)\n",
    "    \n",
    "    # Specificity\n",
    "    tn = np.sum((y_true_binary == 0) & (y_pred_binary == 0))\n",
    "    fp = np.sum((y_true_binary == 0) & (y_pred_binary == 1))\n",
    "    results['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Positive Predictive Value (Precision)\n",
    "    results['ppv'] = precision_score(y_true_binary, y_pred_binary)\n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    tn = np.sum((y_true_binary == 0) & (y_pred_binary == 0))\n",
    "    fn = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n",
    "    results['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    # Number Needed to Screen\n",
    "    tp = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n",
    "    results['nns'] = len(y_pred) / tp if tp > 0 else float('inf')\n",
    "    \n",
    "    # Per-class sensitivity for critical conditions\n",
    "    for cls in positive_classes:\n",
    "        cls_idx = label_encoder.transform([cls])[0]\n",
    "        cls_mask = y_true == cls_idx\n",
    "        if cls_mask.sum() > 0:\n",
    "            cls_recall = recall_score(\n",
    "                y_true[cls_mask] == cls_idx,\n",
    "                y_pred[cls_mask] == cls_idx\n",
    "            )\n",
    "            results[f'{cls}_sensitivity'] = cls_recall\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate clinical metrics\n",
    "clinical_metrics = calculate_clinical_metrics(y_test, best_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLINICAL VALIDITY METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBinary Classification (Any Mental Health Issue vs Neutral):\")\n",
    "print(f\"  Sensitivity (Recall): {clinical_metrics['sensitivity']:.3f}\")\n",
    "print(f\"  Specificity: {clinical_metrics['specificity']:.3f}\")\n",
    "print(f\"  Positive Predictive Value: {clinical_metrics['ppv']:.3f}\")\n",
    "print(f\"  Negative Predictive Value: {clinical_metrics['npv']:.3f}\")\n",
    "print(f\"  Number Needed to Screen: {clinical_metrics['nns']:.1f}\")\n",
    "\n",
    "print(\"\\nPer-Condition Sensitivity:\")\n",
    "for condition in ['depression', 'anxiety', 'suicidal_ideation']:\n",
    "    if f'{condition}_sensitivity' in clinical_metrics:\n",
    "        print(f\"  {condition}: {clinical_metrics[f'{condition}_sensitivity']:.3f}\")\n",
    "\n",
    "# Risk stratification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RISK STRATIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define risk levels\n",
    "risk_mapping = {\n",
    "    'suicidal_ideation': 'CRITICAL',\n",
    "    'depression': 'HIGH',\n",
    "    'anxiety': 'MODERATE',\n",
    "    'neutral': 'LOW'\n",
    "}\n",
    "\n",
    "# Get predicted risk levels\n",
    "predicted_classes = label_encoder.inverse_transform(best_predictions)\n",
    "predicted_risks = [risk_mapping[cls] for cls in predicted_classes]\n",
    "\n",
    "# Count risk distributions\n",
    "risk_counts = Counter(predicted_risks)\n",
    "print(\"\\nPredicted Risk Distribution:\")\n",
    "for risk in ['CRITICAL', 'HIGH', 'MODERATE', 'LOW']:\n",
    "    count = risk_counts.get(risk, 0)\n",
    "    percentage = count / len(predicted_risks) * 100\n",
    "    print(f\"  {risk:10s}: {count:5d} ({percentage:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model and preprocessing pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save components\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'preprocessor': preprocessor,\n",
    "    'feature_engineer': feature_engineer,\n",
    "    'label_encoder': label_encoder,\n",
    "    'scaler': scaler,\n",
    "    'model_name': best_model_name,\n",
    "    'strategy': best_strategy,\n",
    "    'clinical_metrics': clinical_metrics,\n",
    "    'feature_names': feature_names if 'feature_names' in locals() else None\n",
    "}\n",
    "\n",
    "# Save model artifacts\n",
    "joblib.dump(model_artifacts, 'models/mental_health_model.pkl')\n",
    "print(f\"‚úÖ Saved model artifacts to 'models/mental_health_model.pkl'\")\n",
    "\n",
    "# Save results\n",
    "comparison_df.to_csv('models/model_comparison_results.csv', index=False)\n",
    "print(f\"‚úÖ Saved comparison results to 'models/model_comparison_results.csv'\")\n",
    "\n",
    "# Create inference function\n",
    "def predict_mental_health(text, model_artifacts):\n",
    "    \"\"\"Predict mental health status from Arabic text\"\"\"\n",
    "    \n",
    "    # Extract components\n",
    "    model = model_artifacts['model']\n",
    "    preprocessor = model_artifacts['preprocessor']\n",
    "    feature_engineer = model_artifacts['feature_engineer']\n",
    "    label_encoder = model_artifacts['label_encoder']\n",
    "    scaler = model_artifacts['scaler']\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_text = preprocessor.preprocess(text)\n",
    "    \n",
    "    # Extract features\n",
    "    psych_features = preprocessor.extract_features(text)\n",
    "    \n",
    "    # Create TF-IDF features\n",
    "    tfidf_feat = feature_engineer.tfidf_vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Create topic features\n",
    "    topic_feat = nmf_model.transform(tfidf_feat)\n",
    "    \n",
    "    # Combine features\n",
    "    numerical_feat = np.array([list(psych_features.values())])\n",
    "    numerical_feat_scaled = scaler.transform(numerical_feat)\n",
    "    \n",
    "    X = feature_engineer.combine_features(\n",
    "        tfidf_feat,\n",
    "        numerical_feat_scaled,\n",
    "        topic_feat\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(X)[0]\n",
    "    prediction_proba = model.predict_proba(X)[0] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Get class and risk\n",
    "    predicted_class = label_encoder.inverse_transform([prediction])[0]\n",
    "    risk_level = risk_mapping[predicted_class]\n",
    "    \n",
    "    result = {\n",
    "        'text': text,\n",
    "        'processed_text': processed_text,\n",
    "        'prediction': predicted_class,\n",
    "        'risk_level': risk_level,\n",
    "        'confidence': prediction_proba.max() if prediction_proba is not None else None,\n",
    "        'probabilities': {\n",
    "            cls: prob for cls, prob in zip(label_encoder.classes_, prediction_proba)\n",
    "        } if prediction_proba is not None else None,\n",
    "        'features': psych_features\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test inference\n",
    "test_texts = [\n",
    "    \"ÿ£ÿ¥ÿπÿ± ÿ®ÿßŸÑÿ≠ÿ≤ŸÜ ÿßŸÑÿ¥ÿØŸäÿØ ŸàŸÑÿß ÿ£ÿ±ŸäÿØ ÿßŸÑÿÆÿ±Ÿàÿ¨ ŸÖŸÜ ÿßŸÑŸÖŸÜÿ≤ŸÑ\",\n",
    "    \"ÿßŸÑŸäŸàŸÖ ŸÉÿßŸÜ ÿ±ÿßÿ¶ÿπÿßŸã ŸàÿßŸÑÿ≠ŸÖÿØ ŸÑŸÑŸá ÿπŸÑŸâ ŸÉŸÑ ÿ¥Ÿäÿ°\",\n",
    "    \"ÿßŸÑŸÇŸÑŸÇ Ÿäÿ≥Ÿäÿ∑ÿ± ÿπŸÑŸä ŸàŸÑÿß ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑŸÜŸàŸÖ\"\n",
    "]\n",
    "\n",
    "print(\"\\nTest Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_mental_health(text, model_artifacts)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Risk Level: {result['risk_level']}\")\n",
    "    if result['confidence']:\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ethical Considerations and Limitations\n",
    "\n",
    "### Ethical Guidelines:\n",
    "1. **Privacy**: Ensure all data is anonymized and encrypted\n",
    "2. **Consent**: Obtain explicit consent for mental health screening\n",
    "3. **Clinical Integration**: Model should supplement, not replace, professional assessment\n",
    "4. **Crisis Management**: Implement immediate referral pathways for high-risk cases\n",
    "5. **Bias Monitoring**: Regular audits for demographic and linguistic biases\n",
    "\n",
    "### Limitations:\n",
    "1. **Data Quality**: Model performance depends on labeled data quality\n",
    "2. **Cultural Context**: May not generalize across all Arabic dialects/cultures\n",
    "3. **Temporal Validity**: Mental health expressions evolve; model needs updates\n",
    "4. **False Negatives**: Critical for suicidal ideation - requires conservative thresholds\n",
    "5. **Interpretability**: Deep learning models lack full explainability\n",
    "\n",
    "### Recommendations:\n",
    "1. Implement continuous monitoring and model updates\n",
    "2. Establish feedback loops with mental health professionals\n",
    "3. Develop region-specific model variants\n",
    "4. Create comprehensive documentation for clinical users\n",
    "5. Implement A/B testing framework for model improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MENTAL HEALTH DETECTION SYSTEM - FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  Class imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model Performance:\")\n",
    "print(f\"  Model: {best_model_name} ({best_strategy})\")\n",
    "print(f\"  F1 Score (Macro): {comparison_df.nlargest(1, 'Test_F1_Macro')['Test_F1_Macro'].values[0]:.4f}\")\n",
    "print(f\"  F1 Score (Weighted): {comparison_df.nlargest(1, 'Test_F1_Macro')['Test_F1_Weighted'].values[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nüè• Clinical Metrics:\")\n",
    "print(f\"  Sensitivity: {clinical_metrics['sensitivity']:.3f}\")\n",
    "print(f\"  Specificity: {clinical_metrics['specificity']:.3f}\")\n",
    "print(f\"  Suicidal Ideation Detection: {clinical_metrics.get('suicidal_ideation_sensitivity', 0):.3f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Saved Artifacts:\")\n",
    "print(f\"  - Model and pipeline: models/mental_health_model.pkl\")\n",
    "print(f\"  - Results comparison: models/model_comparison_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Research notebook completed successfully!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
